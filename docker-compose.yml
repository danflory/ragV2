services:
  # SERVICE 1: THE BRAIN (Backend)
  rag_app:
    build: .
    container_name: agy_rag_backend
    ports:
      - "5050:5050"
    env_file:
      - .env
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/app/data/.huggingface
      - L1_URL=http://ollama:11434
    volumes:
      - .:/app
      - ./data/huggingface:/app/data/.huggingface
    depends_on:
      - chroma_db
      - postgres_db
      - ollama
    shm_size: '8gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    restart: "no"

  # SERVICE 2: THE MEMORY (Vector Store)
  chroma_db:
    image: chromadb/chroma:latest
    container_name: agy_chroma
    volumes:
      - ./data/chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
    ports:
      - "8000:8000"
    networks:
      default:

        # SERVICE 3: THE AMNESIA FIX (Chat History)
  postgres_db:
    image: postgres:16-alpine
    container_name: agy_postgres
    restart: always
    environment:
      - POSTGRES_USER=${DB_USER:-agy_user}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-agy_pass}
      - POSTGRES_DB=${DB_NAME:-chat_history}
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres_data:/var/lib/postgresql/data
    networks:
      default:

        # SERVICE 4: THE BRAIN (Local Inference)
  ollama:
    image: ollama/ollama:latest
    container_name: agy_ollama
    volumes:
      - ./data/ollama_models:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      default:
