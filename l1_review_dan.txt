import logging
import asyncio
from datetime import datetime
from ollama import AsyncClient, ResponseError
from .config import config

logger = logging.getLogger("AGY_L1")

class L1Driver:
    """
    Modern L1 Driver for AntiGravity.
    Uses native Async API to keep models warm in VRAM.
    Implements Lazy Client Initialization for event loop stability in tests.
    """
    def __init__(self):
        # Do not initialize the client here to avoid binding to the wrong loop
        self._client = None
        self.last_success = datetime.now()

    @property
    def client(self) -> AsyncClient:
        """Property that ensures the client is created within the running event loop."""
        if self._client is None:
            # Library needs the base host (e.g., http://localhost:11434)
            base_host = config.OLLAMA_BASE_URL.replace("/api/generate", "")
            self._client = AsyncClient(host=base_host)
        return self._client

    async def check_model_exists(self) -> bool:
        """
        Robust check of local models.
        Parses the list response safely to prevent 'name' KeyErrors seen in tests.
        """
        try:
            # Access the client property to trigger initialization if needed
            response = await self.client.list()
            
            # Access the list of models safely using the dictionary key
            models_list = response.get('models', [])
            
            # Extract names and look for the config-specified model
            installed_names = [m.get('name') for m in models_list if m.get('name')]
            
            # Direct match check
            if config.MODEL in installed_names:
                return True
            
            # Secondary check: handle cases where 'latest' tag is implicit
            for name in installed_names:
                if name.startswith(config.MODEL.split(':')[0]):
                    return True

            logger.error(f"âŒ Model '{config.MODEL}' not found. Available: {installed_names}")
            return False
            
        except Exception as e:
            logger.error(f"âŒ Connection to Ollama failed: {e}")
            return False

    async def ask_local_llm(self, prompt: str) -> str:
        """
        The primary entry point for Layer 1.
        Features: Identity injection and graceful API escalation.
        """
        # Verification step using our robust model check
        if not await self.check_model_exists():
            return "ESCALATE TO L2"

        try:
            logger.info(f"ðŸ¢ L1 [{config.MODEL}] processing for {config.USER_NAME}...")
            
            # Identity injection for the 'Dan' persona
            system_msg = f"You are an expert AI assistant helping {config.USER_NAME}."
            
            # Native async generation call
            response = await self.client.generate(
                model=config.MODEL,
                prompt=prompt,
                system=system_msg,
                stream=False,
                options={
                    "num_ctx": 4096,
                    "temperature": 0.7,
                    "num_gpu": 1  # Force usage of the TITAN RTX
                }
            )

            self.last_success = datetime.now()
            return response.get("response", "")

        except ResponseError as e:
            logger.warning(f"âš ï¸ L1 API Error: {e.error}. Escalating to L2.")
            return "ESCALATE TO L2"

        except Exception as e:
            logger.error(f"ðŸ”¥ L1 Critical Driver Failure: {e}")
            return "ESCALATE TO L2"

# Instantiate singleton for use in router.py
l1_engine = L1Driver()

# Backwards compatibility helper for existing router calls
async def ask_deepseek(prompt_text: str) -> str:
    """Helper function to route calls to the L1 Engine instance."""
    return await l1_engine.ask_local_llm(prompt_text)